ðŸ“Š Customer Churn Prediction

ðŸ“Œ Project Overview
Customer churn prediction helps businesses identify customers who are likely to discontinue their service, enabling proactive retention strategies. Since retaining customers is more cost-effective than acquiring new ones, predicting churn can significantly improve profitability.

This project builds a machine learning pipeline to predict churn using demographic, behavioral, and transactional data. The final model (XGBoost) achieved 100% recall after addressing class imbalance with SMOTE, making it highly effective for capturing potential churners.

ðŸ“‚ Dataset
Source: Kaggle â€“ Customer Churn Dataset
Size: 505,256 rows Ã— 12 features

Key Features:

CustomerID â€“ Unique customer identifier

Age â€“ Customerâ€™s age

Gender â€“ Male/Female

Tenure â€“ Duration of service usage

Usage Frequency â€“ Monthly service usage

Support Calls â€“ Customer service calls made

Payment Delay â€“ Late bill payments

Subscription Type â€“ Basic / Standard / Premium

Contract Length â€“ Monthly / Quarterly / Annual

Total Spend â€“ Lifetime spend of customer

Last Interaction â€“ Months since last activity

Churn â€“ Target label (1 = churned, 0 = active)

ðŸ›  Steps Performed
1. Data Preprocessing
Handled missing values

Encoded categorical features (Label Encoding & One-Hot Encoding)

Scaled numeric features using StandardScaler

Removed duplicates

Train-validation-test split with Stratified Sampling

2. Exploratory Data Analysis
Age vs Churn: Customers <30 or >60 churn more; 40â€“50 are most loyal

Support Calls: Strong churn indicator; more than 5 calls = higher churn

Payment Delay: Longer delays linked to churn

Subscription & Contract: Premium + Annual contracts â†’ higher retention

Gender: No significant effect

3. Modeling
Baseline Models:

Logistic Regression

Random Forest Classifier

XGBoost Classifier

Evaluation Metrics: Accuracy, Precision, Recall, ROC-AUC, MAE, RMSE, MSE

Model	                 ROC-AUC	  Recall	   MAE	     RMSE
Logistic Regression    	0.82	     98%	     0.21	     0.45
Random Forest	          0.91	     100%	     0.16	     0.36
XGBoost	                0.93	     100%	     0.14	     0.32

4. Addressing Class Imbalance
Applied SMOTE (Synthetic Minority Oversampling Technique)

XGBoost & Random Forest maintained 100% recall on balanced data

ðŸ’¡ Key Insights & Recommendations
Support Calls: High churn indicator â†’ improve first-call resolution

Age Group 40â€“50: Most loyal â†’ offer rewards

Payment Delay: Correlates with churn â†’ send payment reminders

Model Deployment: Use XGBoost in production for real-time churn prediction

ðŸš€ Tech Stack
Python (Pandas, NumPy, Scikit-learn, XGBoost, Matplotlib, Seaborn)

Jupyter Notebook â€“ Interactive development

SMOTE â€“ Class balancing
